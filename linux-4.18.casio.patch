diff --git a/Makefile b/Makefile
index 863f585..6926d34 100644
--- a/Makefile
+++ b/Makefile
@@ -2,7 +2,7 @@
 VERSION = 4
 PATCHLEVEL = 18
 SUBLEVEL = 0
-EXTRAVERSION =
+EXTRAVERSION = .casio
 NAME = Merciless Moray
 
 # *DOCUMENTATION*
diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
index 887d3a7..323d151 100644
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -2937,6 +2937,17 @@ config X86_DMA_REMAP
 config HAVE_GENERIC_GUP
 	def_bool y
 
+# The following entry adds a new configuration option to wrap all SCHED CASIO Linux Scheduler code.
+# The bool directive states that the option entry is a feature and can only be set using two values(y or n).
+# The name "CASIO scheduling policy" provides the name of this option in the various configuration utilities.
+# The default value of this option is set as y.
+menu "CASIO scheduler"
+
+config SCHED_CASIO_POLICY
+        bool "CASIO scheduling policy"
+        default y
+endmenu
+
 source "net/Kconfig"
 
 source "drivers/Kconfig"
diff --git a/fs/proc/Makefile b/fs/proc/Makefile
index ead487e..44f4223 100644
--- a/fs/proc/Makefile
+++ b/fs/proc/Makefile
@@ -27,6 +27,7 @@ proc-y	+= softirqs.o
 proc-y	+= namespaces.o
 proc-y	+= self.o
 proc-y	+= thread_self.o
+proc-y  += proc_misc.o
 proc-$(CONFIG_PROC_SYSCTL)	+= proc_sysctl.o
 proc-$(CONFIG_NET)		+= proc_net.o
 proc-$(CONFIG_PROC_KCORE)	+= kcore.o
diff --git a/fs/proc/proc_misc.c b/fs/proc/proc_misc.c
new file mode 100644
index 0000000..6473485
--- /dev/null
+++ b/fs/proc/proc_misc.c
@@ -0,0 +1,129 @@
+/*
+ *  linux/fs/proc/proc_misc.c
+ *
+ *  linux/fs/proc/array.c
+ *  Copyright (C) 1992  by Linus Torvalds
+ *  based on ideas by Darren Senn
+ *
+ *  This used to be the part of array.c. See the rest of history and credits
+ *  there. I took this into a separate file and switched the thing to generic
+ *  proc_file_inode_operations, leaving in array.c only per-process stuff.
+ *  Inumbers allocation made dynamic (via create_proc_entry()).  AV, May 1999.
+ *
+ * Changes:
+ * Fulton Green      :  Encapsulated position metric calculations.
+ *                      <kernel@FultonGreen.com>
+ */
+
+#include <linux/types.h>
+#include <linux/errno.h>
+#include <linux/time.h>
+#include <linux/kernel.h>
+#include <linux/kernel_stat.h>
+#include <linux/fs.h>
+#include <linux/tty.h>
+#include <linux/string.h>
+#include <linux/mman.h>
+#include <linux/proc_fs.h>
+#include <linux/ioport.h>
+#include <linux/mm.h>
+#include <linux/mmzone.h>
+#include <linux/pagemap.h>
+#include <linux/swap.h>
+#include <linux/slab.h>
+#include <linux/smp.h>
+#include <linux/signal.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/seq_file.h>
+#include <linux/times.h>
+#include <linux/profile.h>
+#include <linux/utsname.h>
+#include <linux/blkdev.h>
+#include <linux/hugetlb.h>
+#include <linux/jiffies.h>
+#include <linux/sysrq.h>
+#include <linux/vmalloc.h>
+#include <linux/crash_dump.h>
+#include <linux/pid_namespace.h>
+#include <asm/uaccess.h>
+#include <asm/pgtable.h>
+#include <asm/io.h>
+#include <asm/tlb.h>
+#include <asm/div64.h>
+#include "internal.h"
+
+#ifdef  CONFIG_SCHED_CASIO_POLICY
+#define CASIO_MAX_CURSOR_LINES_EVENTS   1
+
+static int casio_open(struct inode *inode, struct file *file)
+{
+        return 0;
+}
+
+static ssize_t casio_read(struct file *filp, char __user *buf, size_t count, loff_t *f_pos)
+{
+        char buffer[CASIO_MSG_SIZE];
+        unsigned int len=0,k,i;
+	/*
+	 * Registering the struct casio_event_log in kernel and invocation of casio_event_log function.
+	 */
+        struct casio_event_log *log=NULL;
+        buffer[0]='\0';
+        log=get_casio_event_log();
+        if(log){
+                if(log->cursor < log->lines){
+                        k=(log->lines > (log->cursor + CASIO_MAX_CURSOR_LINES_EVENTS))?(log->cursor + CASIO_MAX_CURSOR_LINES_EVENTS):(log->lines);
+                        for(i=log->cursor; i<k;i++){
+                                len = snprintf(buffer, count, "%s%d,%llu,%s\n",
+                                        buffer,
+                                        log->casio_event[i].action,
+                                        log->casio_event[i].timestamp,
+                                        log->casio_event[i].msg);
+                        }
+                        log->cursor=k;
+                }
+                if(len) 
+                        copy_to_user(buf,buffer,len);
+
+        }
+        return len;
+}
+
+static int casio_release(struct inode *inode, struct file *file)
+{
+        return 0;
+}
+
+/*
+ * The fields are function callbacks to CASIO task operations
+ */
+static const struct file_operations proc_casio_operations = {
+        .open           = casio_open,
+        .read           = casio_read,
+        .release        = casio_release,
+};
+
+/*
+ * Initializes the CASIO operations event.
+ */
+int __init proc_casio_init(void)
+{
+        /*struct proc_dir_entry *casio_entry;
+
+        casio_entry = create_proc_entry("casio_event", 0666, &proc_root);
+
+        if (casio_entry){
+                casio_entry->proc_fops = &proc_casio_operations;
+                casio_entry->data=NULL;
+		return 0;
+        }
+
+	return -1;*/
+
+	proc_create("casio_event", 0, NULL, &proc_casio_operations);
+        return 0;
+}
+module_init(proc_casio_init);
+#endif
+
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 43731fe..8dfe3e9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -591,6 +591,16 @@ struct wake_q_node {
 };
 
 struct task_struct {
+/*
+ * Field casio_id is used to set the logical identifier of a CASIO task.
+ * The relative deadline of each CASIO task is set on the deadline.
+ * Its units are in nanoseconds.
+ */
+#ifdef CONFIG_SCHED_CASIO_POLICY
+        unsigned int casio_id;
+        unsigned long long deadline;
+#endif
+
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 	/*
 	 * For reasons of header soup (see current_thread_info()), this
@@ -1898,4 +1908,42 @@ static inline void rseq_syscall(struct pt_regs *regs)
 
 #endif
 
+/*
+ * MSG_SIZE indicates the maximum message size of the msg string.
+ */
+#ifdef  CONFIG_SCHED_CASIO_POLICY
+
+#define CASIO_MSG_SIZE          400
+#define CASIO_MAX_EVENT_LINES   10000
+
+/*
+ * CASIO_ENQUEUE, CASIO_DEQUEUE, CASIO_CONTEXT_SWITCH, CASIO_MSG are the four actions related to CASIO events.
+ */
+#define CASIO_ENQUEUE           1
+#define CASIO_DEQUEUE           2
+#define CASIO_CONTEXT_SWITCH    3
+#define CASIO_MSG               4
+
+/*
+ * action refers to CASIO related events.
+ * timestamp field refers to the instant time at which the action happened.
+ * msg is the field used to write message for debugging.
+ */
+struct casio_event{
+        int action;
+        unsigned long long timestamp;
+        char msg[CASIO_MSG_SIZE];
+};
+
+struct casio_event_log{
+        struct casio_event casio_event[CASIO_MAX_EVENT_LINES];
+        unsigned long lines;
+        unsigned long cursor;
+};
+void init_casio_event_log(void);
+struct casio_event_log * get_casio_event_log(void);
+void register_casio_event(unsigned long long t, char *m, int a);
+
+#endif
+
 #endif
diff --git a/include/uapi/linux/sched.h b/include/uapi/linux/sched.h
index 22627f8..3168f5b 100644
--- a/include/uapi/linux/sched.h
+++ b/include/uapi/linux/sched.h
@@ -41,6 +41,12 @@
 #define SCHED_IDLE		5
 #define SCHED_DEADLINE		6
 
+/*
+ * The following line declares macro for SCHED CASIO Linux Scheduler.
+ */
+#ifdef CONFIG_SCHED_CASIO_POLICY
+#define SCHED_CASIO             7
+#endif
 /* Can be ORed in to make sure the process is reverted back to SCHED_NORMAL on fork */
 #define SCHED_RESET_ON_FORK     0x40000000
 
diff --git a/include/uapi/linux/sched/types.h b/include/uapi/linux/sched/types.h
index 10fbb80..67e950b 100644
--- a/include/uapi/linux/sched/types.h
+++ b/include/uapi/linux/sched/types.h
@@ -6,6 +6,16 @@
 
 struct sched_param {
 	int sched_priority;
+/*
+ * Modifying sched_param to set the required scheduling parameters.
+ * Field casio_id is used to set the logical identifier of a CASIO task.
+ * The relative deadline of each CASIO task is set on the deadline.
+ * Its units are in nanoseconds.
+ */
+#ifdef  CONFIG_SCHED_CASIO_POLICY
+        unsigned int    casio_id;
+        unsigned long long deadline;
+#endif
 };
 
 #define SCHED_ATTR_SIZE_VER0	48	/* sizeof first published struct */
@@ -70,6 +80,17 @@ struct sched_attr {
 	__u64 sched_runtime;
 	__u64 sched_deadline;
 	__u64 sched_period;
+
+/*
+ * Modifying sched_param to set the required scheduling parameters.
+ * Field casio_id is used to set the logical identifier of a CASIO task.
+ * The relative deadline of each CASIO task is set on the deadline.
+ * Its units are in nanoseconds.
+ */
+#ifdef  CONFIG_SCHED_CASIO_POLICY
+        unsigned int    casio_id;
+        unsigned long long deadline;
+#endif
 };
 
 #endif /* _UAPI_LINUX_SCHED_TYPES_H */
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index fe365c9..12aa749 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -5,6 +5,7 @@
  *
  *  Copyright (C) 1991-2002  Linus Torvalds
  */
+
 #include "sched.h"
 
 #include <linux/nospec.h>
@@ -17,6 +18,8 @@
 #include "../workqueue_internal.h"
 #include "../smpboot.h"
 
+#include <uapi/linux/sched.h>
+
 #define CREATE_TRACE_POINTS
 #include <trace/events/sched.h>
 
@@ -713,6 +716,24 @@ int tg_nop(struct task_group *tg, void *data)
 }
 #endif
 
+/*
+ * The file sched_casio.c which implements CASIO module is included.
+ */
+
+#ifdef  CONFIG_SCHED_CASIO_POLICY
+# include "sched_casio.c"
+#endif
+
+/*
+ * The following lines inform the scheduler core that the casio_sched_class is the highest priority scheduler module.
+ */
+/*
+#ifdef CONFIG_SCHED_CASIO_POLICY
+        #define sched_class_highest (&casio_sched_class)
+#else
+        #define sched_class_highest (&dl_sched_class)
+#endif
+*/
 static void set_load_weight(struct task_struct *p, bool update_load)
 {
 	int prio = p->static_prio - MAX_RT_PRIO;
@@ -3331,6 +3352,7 @@ static inline void schedule_debug(struct task_struct *prev)
 static inline struct task_struct *
 pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
 {
+	printk(KERN_ALERT "\n6. In pick_next_task function\n");
 	const struct sched_class *class;
 	struct task_struct *p;
 
@@ -3410,6 +3432,7 @@ pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
  */
 static void __sched notrace __schedule(bool preempt)
 {
+	printk(KERN_ALERT "\n5. In __schedule function\n");
 	struct task_struct *prev, *next;
 	unsigned long *switch_count;
 	struct rq_flags rf;
@@ -3419,7 +3442,7 @@ static void __sched notrace __schedule(bool preempt)
 	cpu = smp_processor_id();
 	rq = cpu_rq(cpu);
 	prev = rq->curr;
-
+	printk(KERN_ALERT "\n5. In __schedule function (2)\n");
 	schedule_debug(prev);
 
 	if (sched_feat(HRTICK))
@@ -3427,7 +3450,8 @@ static void __sched notrace __schedule(bool preempt)
 
 	local_irq_disable();
 	rcu_note_context_switch(preempt);
-
+	
+	printk(KERN_ALERT "\n5. In __schedule function(3)\n");
 	/*
 	 * Make sure that signal_pending_state()->signal_pending() below
 	 * can't be reordered with __set_current_state(TASK_INTERRUPTIBLE)
@@ -3439,40 +3463,51 @@ static void __sched notrace __schedule(bool preempt)
 	rq_lock(rq, &rf);
 	smp_mb__after_spinlock();
 
+	printk(KERN_ALERT "\n5. In __schedule function(4)\n");
 	/* Promote REQ to ACT */
 	rq->clock_update_flags <<= 1;
 	update_rq_clock(rq);
-
+	printk(KERN_ALERT "\n5. In __schedule function(5)\n");
 	switch_count = &prev->nivcsw;
 	if (!preempt && prev->state) {
+		printk(KERN_ALERT "\n5. In __schedule function(6)\n");
 		if (unlikely(signal_pending_state(prev->state, prev))) {
+			printk(KERN_ALERT "\n5. In __schedule function(7)\n");
 			prev->state = TASK_RUNNING;
 		} else {
+			printk(KERN_ALERT "\n5. In __schedule function(8)\n");
 			deactivate_task(rq, prev, DEQUEUE_SLEEP | DEQUEUE_NOCLOCK);
 			prev->on_rq = 0;
 
 			if (prev->in_iowait) {
+				printk(KERN_ALERT "\n5. In __schedule function(9)\n");
 				atomic_inc(&rq->nr_iowait);
 				delayacct_blkio_start();
 			}
-
+			printk(KERN_ALERT "\n5. In __schedule function(10)\n");
 			/*
 			 * If a worker went to sleep, notify and ask workqueue
 			 * whether it wants to wake up a task to maintain
 			 * concurrency.
 			 */
 			if (prev->flags & PF_WQ_WORKER) {
+				printk(KERN_ALERT "\n5. In __schedule function(11)\n");
 				struct task_struct *to_wakeup;
 
 				to_wakeup = wq_worker_sleeping(prev);
 				if (to_wakeup)
+				{
+					printk(KERN_ALERT "\n5. In __schedule function(12)\n");
 					try_to_wake_up_local(to_wakeup, &rf);
+				}
 			}
+			printk(KERN_ALERT "\n5. In __schedule function(13)\n");
 		}
+		printk(KERN_ALERT "\n5. In __schedule function(14)\n");
 		switch_count = &prev->nvcsw;
 	}
-
 	next = pick_next_task(rq, prev, &rf);
+	printk(KERN_ALERT "\n7. After pick_next_task function call.\n");
 	clear_tsk_need_resched(prev);
 	clear_preempt_need_resched();
 
@@ -3537,6 +3572,7 @@ static inline void sched_submit_work(struct task_struct *tsk)
 
 asmlinkage __visible void __sched schedule(void)
 {
+	//printk(KERN_ALERT "\n2. In schedule function\n");
 	struct task_struct *tsk = current;
 
 	sched_submit_work(tsk);
@@ -3913,7 +3949,7 @@ void set_user_nice(struct task_struct *p, long nice)
 	 * it wont have any effect on scheduling until the task is
 	 * SCHED_DEADLINE, SCHED_FIFO or SCHED_RR:
 	 */
-	if (task_has_dl_policy(p) || task_has_rt_policy(p)) {
+	if (task_has_dl_policy(p) || task_has_rt_policy(p) || task_has_casio_policy(p)) {
 		p->static_prio = NICE_TO_PRIO(nice);
 		goto out_unlock;
 	}
@@ -4106,6 +4142,7 @@ static void __setscheduler_params(struct task_struct *p,
 static void __setscheduler(struct rq *rq, struct task_struct *p,
 			   const struct sched_attr *attr, bool keep_boost)
 {
+	printk(KERN_ALERT "\n3. In __setscheduler function\n");
 	__setscheduler_params(p, attr);
 
 	/*
@@ -4115,11 +4152,18 @@ static void __setscheduler(struct rq *rq, struct task_struct *p,
 	p->prio = normal_prio(p);
 	if (keep_boost)
 		p->prio = rt_effective_prio(p, p->prio);
-
+	
 	if (dl_prio(p->prio))
 		p->sched_class = &dl_sched_class;
 	else if (rt_prio(p->prio))
 		p->sched_class = &rt_sched_class;
+/*
+ * Setting the sched class field of the struct task struct variable that represents the task in the system with the address of new scheduling class variable.
+ */
+#ifdef CONFIG_SCHED_CASIO_POLICY
+        else if ((p->prio) == SCHED_CASIO)
+                p->sched_class = &casio_sched_class;
+#endif
 	else
 		p->sched_class = &fair_sched_class;
 }
@@ -4144,6 +4188,7 @@ static int __sched_setscheduler(struct task_struct *p,
 				const struct sched_attr *attr,
 				bool user, bool pi)
 {
+	printk(KERN_ALERT "\n2. In __sched_setscheduler function\n");
 	int newprio = dl_policy(attr->sched_policy) ? MAX_DL_PRIO - 1 :
 		      MAX_RT_PRIO - 1 - attr->sched_priority;
 	int retval, oldprio, oldpolicy = -1, queued, running;
@@ -4216,6 +4261,9 @@ static int __sched_setscheduler(struct task_struct *p,
 		if (dl_policy(policy))
 			return -EPERM;
 
+		if (casio_policy(policy))
+			return -EPERM;
+
 		/*
 		 * Treat SCHED_IDLE as nice 20. Only allow a switch to
 		 * SCHED_NORMAL if the RLIMIT_NICE would normally permit it.
@@ -4272,13 +4320,23 @@ static int __sched_setscheduler(struct task_struct *p,
 			goto change;
 		if (dl_policy(policy) && dl_param_changed(p, attr))
 			goto change;
+		if (casio_policy(policy))
+			goto change;
 
 		p->sched_reset_on_fork = reset_on_fork;
 		task_rq_unlock(rq, p, &rf);
 		return 0;
 	}
+
 change:
 
+#ifdef CONFIG_SCHED_CASIO_POLICY
+        if(policy==SCHED_CASIO){
+                p->deadline = attr->deadline;
+                p->casio_id = attr->casio_id;
+        }
+#endif
+
 	if (user) {
 #ifdef CONFIG_RT_GROUP_SCHED
 		/*
@@ -4352,6 +4410,13 @@ static int __sched_setscheduler(struct task_struct *p,
 		put_prev_task(rq, p);
 
 	prev_class = p->sched_class;
+
+#ifdef CONFIG_SCHED_CASIO_POLICY
+       if(casio_policy(policy)){
+               add_casio_task_2_list(&rq->casio_rq, p);
+       }
+#endif
+
 	__setscheduler(rq, p, attr, pi);
 
 	if (queued) {
@@ -4386,6 +4451,7 @@ static int __sched_setscheduler(struct task_struct *p,
 static int _sched_setscheduler(struct task_struct *p, int policy,
 			       const struct sched_param *param, bool check)
 {
+	printk(KERN_ALERT "\n4. In _sched_setscheduler function\n");
 	struct sched_attr attr = {
 		.sched_policy   = policy,
 		.sched_priority = param->sched_priority,
@@ -4420,6 +4486,7 @@ EXPORT_SYMBOL_GPL(sched_setscheduler);
 
 int sched_setattr(struct task_struct *p, const struct sched_attr *attr)
 {
+	printk(KERN_ALERT "\n1. In sched_setattr function\n");
 	return __sched_setscheduler(p, attr, true, true);
 }
 EXPORT_SYMBOL_GPL(sched_setattr);
@@ -4742,7 +4809,7 @@ SYSCALL_DEFINE4(sched_getattr, pid_t, pid, struct sched_attr __user *, uattr,
 	attr.sched_policy = p->policy;
 	if (p->sched_reset_on_fork)
 		attr.sched_flags |= SCHED_FLAG_RESET_ON_FORK;
-	if (task_has_dl_policy(p))
+	else if (task_has_dl_policy(p))
 		__getparam_dl(p, &attr);
 	else if (task_has_rt_policy(p))
 		attr.sched_priority = p->rt_priority;
@@ -6027,6 +6094,13 @@ void __init sched_init(void)
 #ifdef CONFIG_FAIR_GROUP_SCHED
 		root_task_group.shares = ROOT_TASK_GROUP_LOAD;
 		INIT_LIST_HEAD(&rq->leaf_cfs_rq_list);
+/*
+ * Since scheduler and run queue are initialized in this function, data fields related to CASIO also needs to be initialized.
+ * Here init_casio_rq function initalizes the casio_rq fields.
+ */
+#ifdef CONFIG_SCHED_CASIO_POLICY
+        	init_casio_rq(&rq->casio_rq);
+#endif
 		rq->tmp_alone_branch = &rq->leaf_cfs_rq_list;
 		/*
 		 * How much CPU bandwidth does root_task_group get?
@@ -6112,6 +6186,13 @@ void __init sched_init(void)
 
 	init_schedstats();
 
+/*
+ * The following function call registers the context switching.
+ */
+#ifdef CONFIG_SCHED_CASIO_POLICY
+        init_casio_event_log();
+#endif
+
 	scheduler_running = 1;
 }
 
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c7742dc..7811fa5 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -141,6 +141,14 @@ static inline void cpu_load_update_active(struct rq *this_rq) { }
 #define NICE_0_LOAD		(1L << NICE_0_LOAD_SHIFT)
 
 /*
+ * These are the 'tuning knobs' of the scheduler:
+ *
+ * default timeslice is 100 msecs (used only for SCHED_RR tasks).
+ * Timeslices get refilled after they expire.
+ */
+#define DEF_TIMESLICE		(100 * HZ / 1000)
+
+/*
  * Single value that decides SCHED_DEADLINE internal math precision.
  * 10 -> just above 1us
  * 9  -> just above 0.5us
@@ -163,6 +171,12 @@ static inline int fair_policy(int policy)
 
 static inline int rt_policy(int policy)
 {
+/*
+ * Included SCHED_CASIO_POLICY as real time class, once SCHED_CASIO has higher priority than SCHED_RR and SCHEDFIFO.
+ */
+#ifdef CONFIG_SCHED_CASIO_POLICY
+        return policy == SCHED_CASIO;
+#endif
 	return policy == SCHED_FIFO || policy == SCHED_RR;
 }
 
@@ -170,8 +184,24 @@ static inline int dl_policy(int policy)
 {
 	return policy == SCHED_DEADLINE;
 }
+
+/*
+ * Check function for CASIO policy
+ */
+static inline int casio_policy(int policy)
+{
+	return policy == SCHED_CASIO;
+}
+
 static inline bool valid_policy(int policy)
 {
+/*
+ * Sets the scheduling policy and scheduling parameters of the CASIO task.
+ */
+#ifdef CONFIG_SCHED_CASIO_POLICY
+	if (casio_policy(policy))
+		return casio_policy(policy);
+#endif
 	return idle_policy(policy) || fair_policy(policy) ||
 		rt_policy(policy) || dl_policy(policy);
 }
@@ -186,6 +216,11 @@ static inline int task_has_dl_policy(struct task_struct *p)
 	return dl_policy(p->policy);
 }
 
+static inline int task_has_casio_policy(struct task_struct *p)
+{
+        return casio_policy(p->policy);
+}
+
 #define cap_scale(v, s) ((v)*(s) >> SCHED_CAPACITY_SHIFT)
 
 /*
@@ -275,6 +310,11 @@ static inline int dl_bandwidth_enabled(void)
 	return sysctl_sched_rt_runtime >= 0;
 }
 
+static inline int casio_bandwidth_enabled(void)
+{
+	return sysctl_sched_rt_runtime >= 0;
+}
+
 struct dl_bw {
 	raw_spinlock_t		lock;
 	u64			bw;
@@ -311,6 +351,8 @@ extern void sched_dl_do_global(void);
 extern int  sched_dl_overflow(struct task_struct *p, int policy, const struct sched_attr *attr);
 extern void __setparam_dl(struct task_struct *p, const struct sched_attr *attr);
 extern void __getparam_dl(struct task_struct *p, struct sched_attr *attr);
+extern void __setparam_casio(struct task_struct *p, const struct sched_attr *attr);
+extern void __getparam_casio(struct task_struct *p, struct sched_attr *attr);
 extern bool __checkparam_dl(const struct sched_attr *attr);
 extern bool dl_param_changed(struct task_struct *p, const struct sched_attr *attr);
 extern int  dl_task_can_attach(struct task_struct *p, const struct cpumask *cs_cpus_allowed);
@@ -745,6 +787,34 @@ extern void rto_push_irq_work_func(struct irq_work *work);
 #endif /* CONFIG_SMP */
 
 /*
+ * The information about CASIO task is stored using struct casio_task data structure.
+ * The data type struct rb_node organizes CASIO tasks on the red-black tree.
+ * The absolute deadline is stored on absolute_deadline field.
+ * struct list_head casio_list_node organizes all CASIO tasks present in a system in a double linked list.
+ * task field is a pointer to the process description entry.
+ * 
+ */
+#ifdef CONFIG_SCHED_CASIO_POLICY
+struct casio_task {
+        struct rb_node casio_rb_node;
+        unsigned long long absolute_deadline;
+        struct list_head casio_list_node;
+        struct task_struct *task;
+};
+/*
+ * Each processor holds a run queue of all runnable processes assigned to it. 
+ * All CASIO tasks assigned to one processor are managed using the struct casio_rq data structure.
+ * The root of red-black tree is the field casio_rb_root.
+ * casio_list_head field is the list head of the data structure.
+ */
+struct casio_rq {
+        struct rb_root casio_rb_root;
+        struct list_head casio_list_head;
+        atomic_t nr_running;
+};
+#endif
+
+/*
  * This is the main, per-CPU runqueue data structure.
  *
  * Locking rule: those places that want to lock multiple runqueues
@@ -752,6 +822,9 @@ extern void rto_push_irq_work_func(struct irq_work *work);
  * acquire operations must be ordered by ascending &runqueue.
  */
 struct rq {
+#ifdef CONFIG_SCHED_CASIO_POLICY
+        struct casio_rq casio_rq;
+#endif
 	/* runqueue lock: */
 	raw_spinlock_t		lock;
 
@@ -1121,7 +1194,7 @@ extern void sched_ttwu_pending(void);
 #define for_each_lower_domain(sd) for (; sd; sd = sd->child)
 
 /**
- * highest_flag_domain - Return highest sched_domain containing flag.
+ * highest_flag_domai - Return highest sched_domain containing flag.
  * @cpu:	The CPU whose highest level of sched domain is to
  *		be returned.
  * @flag:	The flag to check for the highest sched_domain
@@ -1498,6 +1571,19 @@ struct sched_class {
 
 #ifdef CONFIG_SMP
 	int  (*select_task_rq)(struct task_struct *p, int task_cpu, int sd_flag, int flags);
+
+/*	unsigned long (*load_balance) (struct rq *this_rq, int this_cpu,
+			struct rq *busiest, unsigned long max_load_move,
+			struct sched_domain *sd, enum cpu_idle_type idle,
+			int *all_pinned, int *this_best_prio);
+
+	int (*move_one_task) (struct rq *this_rq, int this_cpu,
+			      struct rq *busiest, struct sched_domain *sd,
+			      enum cpu_idle_type idle);
+
+	void (*pre_schedule) (struct rq *this_rq, struct task_struct *task);
+	void (*post_schedule) (struct rq *this_rq);
+*/
 	void (*migrate_task_rq)(struct task_struct *p);
 
 	void (*task_woken)(struct rq *this_rq, struct task_struct *task);
@@ -1547,11 +1633,14 @@ static inline void set_curr_task(struct rq *rq, struct task_struct *curr)
 	curr->sched_class->set_curr_task(rq);
 }
 
-#ifdef CONFIG_SMP
-#define sched_class_highest (&stop_sched_class)
+#ifdef CONFIG_SCHED_CASIO_POLICY
+	#define sched_class_highest (&casio_sched_class)
+#elif CONFIG_SMP
+	#define sched_class_highest (&stop_sched_class)
 #else
-#define sched_class_highest (&dl_sched_class)
+	#define sched_class_highest (&dl_sched_class)
 #endif
+
 #define for_each_class(class) \
    for (class = sched_class_highest; class; class = class->next)
 
@@ -1560,7 +1649,7 @@ extern const struct sched_class dl_sched_class;
 extern const struct sched_class rt_sched_class;
 extern const struct sched_class fair_sched_class;
 extern const struct sched_class idle_sched_class;
-
+extern const struct sched_class casio_sched_class;
 
 #ifdef CONFIG_SMP
 
@@ -1633,6 +1722,22 @@ extern void post_init_entity_util_avg(struct sched_entity *se);
 extern bool sched_can_stop_tick(struct rq *rq);
 extern int __init sched_tick_offload_init(void);
 
+#ifdef CONFIG_CASIO_SCHED_POLICY
+extern void init_casio_rq(struct casio_rq *casio_rq)
+extern void add_casio_task_2_list(struct casio_rq *rq, struct task_struct *p);
+extern void rem_casio_task_list(struct casio_rq *rq, struct task_struct *p);
+extern void remove_casio_task_rb_tree(struct casio_rq *rq, struct casio_task *p);
+extern void insert_casio_task_rb_tree(struct casio_rq *rq, struct casio_task *p);
+#endif
+
+/*
+ * The file sched_casio.c which implements CASIO module is included.
+ */
+/*
+#ifdef  CONFIG_SCHED_CASIO_POLICY
+# include "sched_casio.c"
+#endif
+*/
 /*
  * Tick may be needed by tasks in the runqueue depending on their policy and
  * requirements. If tick is needed, lets send the target an IPI to kick it out of
diff --git a/kernel/sched/sched_casio.c b/kernel/sched/sched_casio.c
new file mode 100644
index 0000000..608f2eb
--- /dev/null
+++ b/kernel/sched/sched_casio.c
@@ -0,0 +1,441 @@
+/*
+ * casio-task scheduling class.
+ *
+ * 
+ */
+/*
+ * log functions.
+ */
+
+//#include "sched.h"
+
+struct casio_event_log casio_event_log;
+
+struct casio_event_log * get_casio_event_log(void)
+{
+	return &casio_event_log;
+}
+void init_casio_event_log(void)
+{
+	char msg[CASIO_MSG_SIZE];
+	casio_event_log.lines=casio_event_log.cursor=0;
+	snprintf(msg,CASIO_MSG_SIZE,"init_casio_event_log:(%lu:%lu)", casio_event_log.lines, casio_event_log.cursor); 
+	register_casio_event(sched_clock(), msg, CASIO_MSG);
+
+}
+void register_casio_event(unsigned long long t, char *m, int a)
+{
+
+	if(casio_event_log.lines < CASIO_MAX_EVENT_LINES){
+		casio_event_log.casio_event[casio_event_log.lines].action=a;
+		casio_event_log.casio_event[casio_event_log.lines].timestamp=t;
+		strncpy(casio_event_log.casio_event[casio_event_log.lines].msg,m,CASIO_MSG_SIZE-1);
+		casio_event_log.lines++;
+	}
+	else{
+		printk(KERN_ALERT "register_casio_event: full\n");
+	}
+
+}
+/*
+ *casio tasks and casio rq
+ */
+void init_casio_rq(struct casio_rq *casio_rq)
+{
+	casio_rq->casio_rb_root=RB_ROOT;
+	INIT_LIST_HEAD(&casio_rq->casio_list_head);
+	atomic_set(&casio_rq->nr_running,0);
+}
+void add_casio_task_2_list(struct casio_rq *rq, struct task_struct *p)
+{
+	struct list_head *ptr=NULL;
+	struct casio_task *new=NULL, *casio_task=NULL;
+	char msg[CASIO_MSG_SIZE];
+	if(rq && p){
+		new=(struct casio_task *) kzalloc(sizeof(struct casio_task),GFP_KERNEL);
+		if(new){
+			casio_task=NULL;
+			new->task=p;
+			new->absolute_deadline=0;
+			list_for_each(ptr,&rq->casio_list_head){
+				casio_task=list_entry(ptr,struct casio_task, casio_list_node);
+				if(casio_task){
+					if(new->task->casio_id < casio_task->task->casio_id){
+						list_add(&new->casio_list_node,ptr);
+					}
+				}
+			}
+			list_add(&new->casio_list_node,&rq->casio_list_head);
+			snprintf(msg,CASIO_MSG_SIZE,"add_casio_task_2_list: %d:%d:%llu",new->task->casio_id,new->task->pid,new->absolute_deadline); 
+			register_casio_event(sched_clock(), msg, CASIO_MSG);
+		}
+		else{
+			printk(KERN_ALERT "add_casio_task_2_list: kzalloc\n");
+		}
+	}
+	else{
+		printk(KERN_ALERT "add_casio_task_2_list: null pointers\n");
+	}
+}
+struct casio_task * find_casio_task_list(struct casio_rq *rq, struct task_struct *p)
+{
+	struct list_head *ptr=NULL;
+	struct casio_task *casio_task=NULL;
+	if(rq && p){
+		list_for_each(ptr,&rq->casio_list_head){
+			casio_task=list_entry(ptr,struct casio_task, casio_list_node);
+			if(casio_task){
+				if(casio_task->task->casio_id == p->casio_id){
+					return casio_task;
+				}
+			}
+		}
+	}
+	return NULL;
+}
+void rem_casio_task_list(struct casio_rq *rq, struct task_struct *p)
+{
+	struct list_head *ptr=NULL,*next=NULL;
+	struct casio_task *casio_task=NULL;
+	char msg[CASIO_MSG_SIZE];
+	if(rq && p){
+		list_for_each_safe(ptr,next,&rq->casio_list_head){
+			casio_task=list_entry(ptr,struct casio_task, casio_list_node);
+			if(casio_task){
+				if(casio_task->task->casio_id == p->casio_id){
+					list_del(ptr);
+					snprintf(msg,CASIO_MSG_SIZE,"rem_casio_task_list: %d:%d:%llu",casio_task->task->casio_id,casio_task->task->pid,casio_task->absolute_deadline); 
+					register_casio_event(sched_clock(), msg, CASIO_MSG);
+					kfree(casio_task);
+					return;
+				}
+			}
+		}
+	}
+}
+/*
+ * rb_tree functions.
+ */
+
+void remove_casio_task_rb_tree(struct casio_rq *rq, struct casio_task *p)
+{
+	rb_erase(&(p->casio_rb_node),&(rq->casio_rb_root));
+	p->casio_rb_node.rb_left=p->casio_rb_node.rb_right=NULL;
+}
+void insert_casio_task_rb_tree(struct casio_rq *rq, struct casio_task *p)
+{
+	struct rb_node **node=NULL;
+	struct rb_node *parent=NULL;
+	struct casio_task *entry=NULL;
+	node=&rq->casio_rb_root.rb_node;
+	while(*node!=NULL){
+		parent=*node;
+		entry=rb_entry(parent, struct casio_task,casio_rb_node);
+		if(entry){
+			if(p->absolute_deadline < entry->absolute_deadline){
+				node=&parent->rb_left;
+			}else{
+				node=&parent->rb_right;
+			}
+		}
+	}
+	rb_link_node(&p->casio_rb_node,parent,node);
+	rb_insert_color(&p->casio_rb_node,&rq->casio_rb_root);
+}
+struct casio_task * earliest_deadline_casio_task_rb_tree(struct casio_rq *rq)
+{
+	struct rb_node *node=NULL;
+	struct casio_task *p=NULL;
+	node=rq->casio_rb_root.rb_node;
+	if(node==NULL)
+		return NULL;
+
+	while(node->rb_left!=NULL){
+		node=node->rb_left;
+	}
+	p=rb_entry(node, struct casio_task,casio_rb_node);
+	return p;
+}
+
+/*
+ * check_preempt_curr_casio function checks whether the the current running task has to be preempted. 
+ * It is invoked followed by the enqueuing or dequeuing of a task.
+ */
+static void check_preempt_curr_casio(struct rq *rq, struct task_struct *p, int flags)
+{
+	struct casio_task *t=NULL,*curr=NULL;
+	if(rq->curr->policy!=SCHED_CASIO){
+		resched_curr(rq);
+	}
+	else{
+		t=earliest_deadline_casio_task_rb_tree(&rq->casio_rq);
+		if(t){
+			curr=find_casio_task_list(&rq->casio_rq,rq->curr);
+			if(curr){
+				if(t->absolute_deadline < curr->absolute_deadline)
+					resched_curr(rq);
+			}
+			else{
+				printk(KERN_ALERT "check_preempt_curr_casio\n");
+			}
+		}
+	}
+}
+
+/*
+ * pick_next_task_casio function selects the task to be executed by the processor.
+ * It is invoked by the scheduler core whenever the currently running task is marked to be preempted.
+ */
+static struct task_struct *pick_next_task_casio(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
+{
+	struct casio_task *t=NULL;
+	t=earliest_deadline_casio_task_rb_tree(&rq->casio_rq);
+	if(t){
+		return t->task;
+	}
+	return NULL;
+}
+
+/*
+ * The enqueue_task_casio is called whenever a CASIO task enters in a runnable way. It receives two pointers.
+ * One pointer is for the run queue of the processor, another for the task that is entering in a runnable state.
+ * It updates the absolute deadline and insert the casio_task on the red-black tree.
+ */
+static void enqueue_task_casio(struct rq *rq, struct task_struct *p, int wakeup)
+{
+	struct casio_task *t=NULL;
+	char msg[CASIO_MSG_SIZE];
+	if(p){
+		t=find_casio_task_list(&rq->casio_rq,p);
+		if(t){
+			t->absolute_deadline=sched_clock()+p->deadline;
+			insert_casio_task_rb_tree(&rq->casio_rq, t);
+			atomic_inc(&rq->casio_rq.nr_running);
+			snprintf(msg,CASIO_MSG_SIZE,"(%d:%d:%llu)",p->casio_id,p->pid,t->absolute_deadline); 
+			register_casio_event(sched_clock(), msg, CASIO_ENQUEUE);
+		}
+		else{
+			printk(KERN_ALERT "enqueue_task_casio\n");
+		}
+	}
+}
+
+/*
+ * dequeue_task_casio is called when the CASIO task is no longer runnable and undoes the work of enqueue_task_casio function.
+ */
+static void dequeue_task_casio(struct rq *rq, struct task_struct *p, int sleep)
+{
+	struct casio_task *t=NULL;
+	char msg[CASIO_MSG_SIZE];
+	if(p){
+		t=find_casio_task_list(&rq->casio_rq,p);
+		if(t){
+			snprintf(msg,CASIO_MSG_SIZE,"(%d:%d:%llu)",t->task->casio_id,t->task->pid,t->absolute_deadline); 
+			register_casio_event(sched_clock(), msg, CASIO_DEQUEUE);	
+			remove_casio_task_rb_tree(&rq->casio_rq, t);
+			atomic_dec(&rq->casio_rq.nr_running);
+			if(t->task->state==TASK_DEAD || t->task->state==EXIT_DEAD || t->task->state==EXIT_ZOMBIE){
+				rem_casio_task_list(&rq->casio_rq,t->task);
+			}
+		}
+		else{
+			printk(KERN_ALERT "dequeue_task_casio\n");
+		}
+	}
+
+}
+
+static void put_prev_task_casio(struct rq *rq, struct task_struct *prev)
+{
+
+}
+
+#ifdef CONFIG_SMP
+static unsigned long load_balance_casio(struct rq *this_rq, int this_cpu, struct rq *busiest,
+		  unsigned long max_load_move,
+		  struct sched_domain *sd, enum cpu_idle_type idle,
+		  int *all_pinned, int *this_best_prio)
+{
+	return 0;
+}
+
+static int move_one_task_casio(struct rq *this_rq, int this_cpu, struct rq *busiest,
+		   struct sched_domain *sd, enum cpu_idle_type idle)
+{
+	return 0;
+}
+#endif
+
+static void task_tick_casio(struct rq *rq, struct task_struct *p, int queued)
+{
+	//check_preempt_curr_casio(rq, p);
+}
+
+static void set_curr_task_casio(struct rq *rq)
+{
+
+}
+
+
+/*
+ * When switching a task to RT, we may overload the runqueue
+ * with RT tasks. In this case we try to push them off to
+ * other runqueues.
+ */
+static void switched_to_casio(struct rq *rq, struct task_struct *p)
+{
+        /*
+         * If we are already running, then there's nothing
+         * that needs to be done. But if we are not running
+         * we may need to preempt the current running task.
+         * If that current running task is also an RT task
+         * then see if we can move to another run queue.
+         */
+}
+
+
+unsigned int get_rr_interval_casio(struct rq *rq, struct task_struct *task)
+{
+	/*
+         * Time slice is 0 for SCHED_FIFO tasks
+         */
+        if (task->policy == SCHED_RR)
+                return DEF_TIMESLICE;
+        else
+                return 0;
+}
+
+static void yield_task_casio(struct rq *rq)
+{
+
+}
+
+
+/*
+ * Priority of the task has changed. This may cause
+ * us to initiate a push or pull.
+ */
+static void prio_changed_casio(struct rq *rq, struct task_struct *p,
+			    int oldprio)
+{
+
+}
+
+static int select_task_rq_casio(struct task_struct *p, int cpu, int sd_flag, int flags)
+{
+
+//	struct rq *rq = task_rq(p);
+
+	if (sd_flag != SD_BALANCE_WAKE)
+		return smp_processor_id();
+
+	return task_cpu(p);
+}
+
+
+static void set_cpus_allowed_casio(struct task_struct *p,
+				const struct cpumask *new_mask)
+{
+
+}
+
+/* Assumes rq->lock is held */
+static void rq_online_casio(struct rq *rq)
+{
+
+}
+
+/* Assumes rq->lock is held */
+static void rq_offline_casio(struct rq *rq)
+{
+
+}
+
+static void pre_schedule_casio(struct rq *rq, struct task_struct *prev)
+{
+
+}
+
+static void post_schedule_casio(struct rq *rq)
+{
+
+}
+/*
+ * If we are not running and we are not going to reschedule soon, we should
+ * try to push tasks away now
+ */
+static void task_woken_casio(struct rq *rq, struct task_struct *p)
+{
+/*        if (!task_running(rq, p) &&
+            !test_tsk_need_resched(rq->curr) &&
+            has_pushable_tasks(rq) &&
+            p->rt.nr_cpus_allowed > 1)
+                push_rt_tasks(rq);
+*/
+}
+
+/*
+ * When switch from the rt queue, we bring ourselves to a position
+ * that we might want to pull RT tasks from other runqueues.
+ */
+static void switched_from_casio(struct rq *rq, struct task_struct *p)
+{
+
+}
+
+void __setparam_casio(struct task_struct *p, const struct sched_attr *attr)
+{
+	p->deadline = attr->deadline;
+        p->casio_id = attr->casio_id;
+}
+
+void __getparam_casio(struct task_struct *p, struct sched_attr *attr)
+{
+	attr->deadline = p->deadline;
+        attr->casio_id = p->casio_id;
+}
+
+/*
+ * Simple, special scheduling class for the per-CPU casio tasks:
+ * Class for implementing the CASIO module.
+ * .next is a pointer to sched_class that is used to organize scheduler modules by priority in a linked list.
+ * The rest of the parameters are functions that act as callbacks to specific events.
+ */
+const struct sched_class casio_sched_class = {
+#ifdef CONFIG_SMP
+	.next 			= &stop_sched_class,
+#else
+	.next			= &dl_sched_class,
+#endif
+	.enqueue_task		= enqueue_task_casio,
+	.dequeue_task		= dequeue_task_casio,
+
+	.check_preempt_curr	= check_preempt_curr_casio,
+
+//	.pick_next_task		= pick_next_task_casio,
+	.put_prev_task		= put_prev_task_casio,
+
+#ifdef CONFIG_SMP
+//	.load_balance		= load_balance_casio,
+//	.move_one_task		= move_one_task_casio,
+
+	.select_task_rq		= select_task_rq_casio,
+//	.set_cpus_allowed       = set_cpus_allowed_casio,
+//	.rq_online              = rq_online_casio,
+//	.rq_offline             = rq_offline_casio,
+//	.pre_schedule		= pre_schedule_casio,
+//	.post_schedule		= post_schedule_casio,
+//	.task_woken		= task_woken_casio,
+//	.switched_from		= switched_from_casio,
+#endif
+
+//	.set_curr_task          = set_curr_task_casio,
+//	.task_tick		= task_tick_casio,
+
+//	.switched_to		= switched_to_casio,
+
+//	.yield_task		= yield_task_casio,
+	.get_rr_interval	= get_rr_interval_casio,
+
+//	.prio_changed		= prio_changed_casio,
+};
